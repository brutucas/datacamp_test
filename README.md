# Text Generation with Hugging Face
### by Benjamin Lucas

As part of the interview process for **DataCamp Content Developer**, I have completed an assignment to create an example course, *'Text Generation with Hugging Face'*. 

Here are two screencast recordings:

- [Lesson 2.1 Screencast: 'Basic GPT-2 Text Generation'](https://drive.google.com/file/d/1EImbyjgDfjzwVATpGwackk0fbkrORR39/view?usp=sharing)
- [Lesson 2.2 Screencast: 'Interactive Storytelling with GPT-2'](https://drive.google.com/file/d/1y5FNcYwFrUSakHdSvrcbYCaVTAYt_TD1/view?usp=drive_link)

^ Please note: I needed to keep within the 4-minute limit, so divided the resource I created into *two lesson recordings* and include them both for your convenience.

My personal learning has already benefitted from these courses, but as part of my research for this course proposal, I was further impressed by *'Working with Hugging Face'* and *'Deep Learning for Text with Pytorch'*. 

However, I have identified more advanced text generation with pre-trained transformer models (undoubtedly a cutting-edge 'frontier' of AI) as an area where DataCamp could offer more content. 

Indeed, this course is designed to fill a gap that I need as a learner, in order to be able to confidently generate a wide variety, complexity, and length of text with pre-trained models, especially in different use-case scenarios.
***
### Course Description:
This course explores the use of Hugging Face Transformers, a leading tool in natural language processing (NLP), for text generation. 

Students will learn to implement, fine-tune, and deploy various transformer models to produce human-like text, exploring both the technical implementations and creative applications.

To complete this course, you would benefit from having already completed ‘Deep Learning for Text with PyTorch’ and ‘Working with Hugging Face’. 

Upon finishing this course, you will have much more confidence using the Hugging Face state-of-the-art ‘Transformers’ library to complete challenging ‘Text Generation’ tasks. 

It will also serve as a helpful stepping stone to the larger ‘Introduction to LLMs in Python’ course.
***
### Course Objectives:
What will this course help you to achieve? For a more streamlined list of objectives, please refer to the course outline. You will see below an exhaustive list of the course objectives. 

It is very long - much longer than anything I would put into a guide for learners. But I include it *as documentation for this interview assignment*, because the too-long list demonstrates very clearly the progress that a DataCamp learner will have made upon completion of this course.

- Identify the shift from traditional machine learning techniques to deep learning in NLP (Lesson 1.1).
- Discuss key innovations such as word embeddings and RNNs (Lesson 1.1).
- Describe the role of word embeddings and recurrent neural networks (RNNs) in advanced NLP (Lesson 1.1).
- Understand the components of the Hugging Face ecosystem (Lesson 1.2).
- Explain how Hugging Face components integrate to facilitate NLP tasks (Lesson 1.2).
- Identify and utilize the core components of the Hugging Face library (Lesson 1.2).
- Integrate these tools to successfully perform complex NLP tasks (Lesson 1.2).
- Set up a Python development environment tailored for NLP applications (Lesson 1.3).
- Utilize pre-trained models for initial text generation and classification tasks (Lesson 1.3).
- Deploy an out-of-the-box GPT-2 model to generate text (Lesson 2.1).
- Understand basic GPT-2 functionality and limitations (Lesson 2.1).
- Create interactive, adaptive, and engaging stories with text-generation (Lesson 2.1).
- Prepare and preprocess datasets specifically tailored for fine-tuning GPT-2 (Lesson 2.3).
- Adjust and optimize hyperparameters for fine-tuning GPT-2 to improve the model’s performance (Lesson 2.3).
- Apply transfer learning techniques to enhance GPT-2’s genre-specific text generation (Lesson 2.3).
- Describe the unique architecture and features of Llama and Mistral models (Lesson 2.4).
- Compare Llama and Mistral models to traditional GPT architectures (Lesson 2.4).
- Set up and deploy Llama or Mistral models for various AI applications (Lesson 2.4).
- Consider the specific strengths and use cases of these different transformer models (Lesson 2.4).
- Understand and explain the necessity of quantization in machine learning (Lesson 3.1).
- Apply both post-training quantization and quantization-aware training techniques (Lesson 3.1).
- Quantize AI models effectively, while maintaining performance considerations (Lesson 3.1).
- Understand the fundamentals of fine-tuning to distinguish it from training a model from scratch (Lesson 3.2).
- Prepare datasets for fine-tuning by selecting, cleaning, and labeling them appropriately (Lesson 3.2).
- Set hyperparameters and use evaluation metrics to effectively measure and troubleshoot the fine-tuning process (Lesson 3.2).
- Design and implement a custom writing assistant to generate specific text (Lesson 3.3).
- Refine generated text to meet qualitative standards and user-specific criteria (Lesson 3.3).
- Integrate user feedback to continuously improve the writing assistant’s performance (Lesson 3.3).
- Develop advanced conversational agents using transformer-based models (Lesson 4.1).
- Incorporate techniques that maintain context across interactions (Lesson 4.1).
- Deliver a seamless and coherent user experience (Lesson 4.1).
- Use quantitative metrics like BLEU and ROUGE to evaluate text generation models (Lesson 4.2).
- Apply qualitative techniques to further refine and optimize AI text generation models (Lesson 4.2).
- Integrate quantitative and qualitative metrics for model evaluation and optimization (Lesson 4.2).
- Recognize biases in AI-generated text and understand their ethical implications (Lesson 4.3).
- Apply strategies to mitigate biases in AI-generated content to enhance fairness and inclusivity (Lesson 4.3).
- Evaluate and adjust AI models to promote ethical practices in content generation (Lesson 4.3).
- Compare models of different sizes and inference speeds (Lesson 4.4).
- Understand relevant metrics for selection, such as tokens per minute (Lesson 4.4).
- Evaluate AI models using TensorFlow or PyTorch (Lesson 4.4).
- Deploy AI models using Flask or FastAPI and implement security features (Lesson 4.4).
